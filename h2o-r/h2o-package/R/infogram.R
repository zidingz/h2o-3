# This file is auto-generated by h2o-3/h2o-bindings/bin/gen_R.py
# Copyright 2016 H2O.ai;  Apache License Version 2.0 (see LICENSE for details) 
#'
# -------------------------- Calculate Infogram -------------------------- #
#'
#' Given a protected_columns list, Infogram will add all predictors that contains information on the 
#'  protected predictors to the protected_columns list.  It will return a set of predictors that
#'  do not contain information on the sensitive/unfair list and hence user can build a fair model.  If no 
#'  protected_columns list is given, Infogram will return a list of core predictors that should be used to build a final model.
#'  Infogram can significantly cut down the number of predictors needed to build a model and hence will build a simple
#'  model that is more interpretable, less susceptible to overfitting, runs faster while providing similar accuracy
#'  as models built using all attributes.
#' 
#'
#' @param x (Optional) A vector containing the names or indices of the predictor variables to use in building the model.
#'        If x is missing, then all columns except y are used.
#' @param y The name or column index of the response variable in the data. 
#'        The response must be either a numeric or a categorical/factor variable. 
#'        If the response is numeric, then a regression model will be trained, otherwise it will train a classification model.
#' @param training_frame Id of the training data frame.
#' @param model_id Destination id for this model; auto-generated if not specified.
#' @param validation_frame Id of the validation data frame.
#' @param seed Seed for random numbers (affects certain parts of the algo that are stochastic and those might or might not be enabled by default).
#'        Defaults to -1 (time-based random number).
#' @param keep_cross_validation_models \code{Logical}. Whether to keep the cross-validation models. Defaults to TRUE.
#' @param keep_cross_validation_predictions \code{Logical}. Whether to keep the predictions of the cross-validation models. Defaults to FALSE.
#' @param keep_cross_validation_fold_assignment \code{Logical}. Whether to keep the cross-validation fold assignment. Defaults to FALSE.
#' @param fold_assignment Cross-validation fold assignment scheme, if fold_column is not specified. The 'Stratified' option will
#'        stratify the folds based on the response variable, for classification problems. Must be one of: "AUTO",
#'        "Random", "Modulo", "Stratified". Defaults to AUTO.
#' @param fold_column Column with cross-validation fold index assignment per observation.
#' @param ignore_const_cols \code{Logical}. Ignore constant columns. Defaults to TRUE.
#' @param score_each_iteration \code{Logical}. Whether to score during each iteration of model training. Defaults to FALSE.
#' @param offset_column Offset column. This will be added to the combination of columns before applying the link function.
#' @param weights_column Column with observation weights. Giving some observation a weight of zero is equivalent to excluding it from
#'        the dataset; giving an observation a relative weight of 2 is equivalent to repeating that row twice. Negative
#'        weights are not allowed. Note: Weights are per-row observation weights and do not increase the size of the
#'        data frame. This is typically the number of times a row is repeated, but non-integer values are supported as
#'        well. During training, rows with higher weights matter more, due to the larger loss function pre-factor. If
#'        you set weight = 0 for a row, the returned prediction frame at that row is zero and this is incorrect. To get
#'        an accurate prediction, remove all rows with weight == 0.
#' @param standardize \code{Logical}. Standardize numeric columns to have zero mean and unit variance Defaults to FALSE.
#' @param distribution Distribution function Must be one of: "AUTO", "bernoulli", "multinomial", "gaussian", "poisson", "gamma",
#'        "tweedie", "laplace", "quantile", "huber". Defaults to AUTO.
#' @param plug_values Plug Values (a single row frame containing values that will be used to impute missing values of the
#'        training/validation frame, use with conjunction missing_values_handling = PlugValues)
#' @param max_iterations Maximum number of iterations Defaults to 0.
#' @param stopping_rounds Early stopping based on convergence of stopping_metric. Stop if simple moving average of length k of the
#'        stopping_metric does not improve for k:=stopping_rounds scoring events (0 to disable) Defaults to 0.
#' @param stopping_metric Metric to use for early stopping (AUTO: logloss for classification, deviance for regression and
#'        anonomaly_score for Isolation Forest). Note that custom and custom_increasing can only be used in GBM and DRF
#'        with the Python client. Must be one of: "AUTO", "deviance", "logloss", "MSE", "RMSE", "MAE", "RMSLE", "AUC",
#'        "AUCPR", "lift_top_group", "misclassification", "mean_per_class_error", "custom", "custom_increasing".
#'        Defaults to AUTO.
#' @param stopping_tolerance Relative tolerance for metric-based stopping criterion (stop if relative improvement is not at least this
#'        much) Defaults to 0.001.
#' @param balance_classes \code{Logical}. Balance training data class counts via over/under-sampling (for imbalanced data). Defaults to
#'        FALSE.
#' @param class_sampling_factors Desired over/under-sampling ratios per class (in lexicographic order). If not specified, sampling factors will
#'        be automatically computed to obtain class balance during training. Requires balance_classes.
#' @param max_after_balance_size Maximum relative size of the training data after balancing class counts (can be less than 1.0). Requires
#'        balance_classes. Defaults to 5.0.
#' @param max_runtime_secs Maximum allowed runtime in seconds for model training. Use 0 to disable. Defaults to 0.
#' @param custom_metric_func Reference to custom evaluation function, format: `language:keyName=funcName`
#' @param auc_type Set default multinomial AUC type. Must be one of: "AUTO", "NONE", "MACRO_OVR", "WEIGHTED_OVR", "MACRO_OVO",
#'        "WEIGHTED_OVO". Defaults to AUTO.
#' @param infogram_algorithm Machine learning algorithm chosen to build the infogram.  AUTO default to GBM Must be one of: "AUTO",
#'        "deeplearning", "drf", "gbm", "glm", "xgboost". Defaults to gbm.
#' @param infogram_algorithm_params parameters specified to the chosen algorithm can be passed to infogram using algorithm_params
#' @param protected_columns predictors that are to be excluded from model due to them being discriminatory or inappropriate for whatever
#'        reason.
#' @param cmi_threshold conditional information threshold between 0 and 1 that is used to decide whether a predictor's conditional
#'        information is high enough to be chosen into the admissible feature set.  Default to 0.1 Defaults to 0.1.
#' @param relevance_threshold relevance threshold between 0 and 1 that is used to decide whether a predictor's relevance level is high
#'        enough to be chosen into the admissible feature set.  Default to 0.1 Defaults to 0.1.
#' @param data_fraction fraction of training frame to use to build the infogram model.  Default to 1.0 Defaults to 1.
#' @param top_n_features number of top k variables to consider based on the varimp.  Default to 0.0 which is to consider all predictors
#'        Defaults to 50.
#' @param compute_p_values \code{Logical}. If true will calculate the p-value. Default to false Defaults to FALSE.
#' @examples
#' \dontrun{
#' h2o.init()
#' 
#' # Run infogram of CAPSULE ~ AGE + RACE + PSA + DCAPS
#' prostate_path <- system.file("extdata", "prostate.csv", package = "h2o")
#' prostate <- h2o.uploadFile(path = prostate_path)
#' prostate$CAPSULE <- as.factor(prostate$CAPSULE)
#' h2o.infogram(y = "CAPSULE", x = c("RACE", "AGE", "PSA", "DCAPS"), training_frame = prostate, distribution = "bernoulli")
#' 
#' }
#' @export
h2o.infogram <- function(x,
                         y,
                         training_frame,
                         model_id = NULL,
                         validation_frame = NULL,
                         seed = -1,
                         keep_cross_validation_models = TRUE,
                         keep_cross_validation_predictions = FALSE,
                         keep_cross_validation_fold_assignment = FALSE,
                         fold_assignment = c("AUTO", "Random", "Modulo", "Stratified"),
                         fold_column = NULL,
                         ignore_const_cols = TRUE,
                         score_each_iteration = FALSE,
                         offset_column = NULL,
                         weights_column = NULL,
                         standardize = FALSE,
                         distribution = c("AUTO", "bernoulli", "multinomial", "gaussian", "poisson", "gamma", "tweedie", "laplace", "quantile", "huber"),
                         plug_values = NULL,
                         max_iterations = 0,
                         stopping_rounds = 0,
                         stopping_metric = c("AUTO", "deviance", "logloss", "MSE", "RMSE", "MAE", "RMSLE", "AUC", "AUCPR", "lift_top_group", "misclassification", "mean_per_class_error", "custom", "custom_increasing"),
                         stopping_tolerance = 0.001,
                         balance_classes = FALSE,
                         class_sampling_factors = NULL,
                         max_after_balance_size = 5.0,
                         max_runtime_secs = 0,
                         custom_metric_func = NULL,
                         auc_type = c("AUTO", "NONE", "MACRO_OVR", "WEIGHTED_OVR", "MACRO_OVO", "WEIGHTED_OVO"),
                         infogram_algorithm = c("AUTO", "deeplearning", "drf", "gbm", "glm", "xgboost"),
                         infogram_algorithm_params = NULL,
                         protected_columns = NULL,
                         cmi_threshold = 0.1,
                         relevance_threshold = 0.1,
                         data_fraction = 1,
                         top_n_features = 50,
                         compute_p_values = FALSE)
{
  # Validate required training_frame first and other frame args: should be a valid key or an H2OFrame object
  training_frame <- .validate.H2OFrame(training_frame, required=TRUE)
  validation_frame <- .validate.H2OFrame(validation_frame, required=FALSE)

  # Validate other required args
  # If x is missing, then assume user wants to use all columns as features.
  if (missing(x)) {
     if (is.numeric(y)) {
         x <- setdiff(col(training_frame), y)
     } else {
         x <- setdiff(colnames(training_frame), y)
     }
  }

  # Build parameter list to send to model builder
  parms <- list()
  parms$training_frame <- training_frame
  args <- .verify_dataxy(training_frame, x, y)
  if( !missing(offset_column) && !is.null(offset_column))  args$x_ignore <- args$x_ignore[!( offset_column == args$x_ignore )]
  if( !missing(weights_column) && !is.null(weights_column)) args$x_ignore <- args$x_ignore[!( weights_column == args$x_ignore )]
  if( !missing(fold_column) && !is.null(fold_column)) args$x_ignore <- args$x_ignore[!( fold_column == args$x_ignore )]
  parms$ignored_columns <- args$x_ignore
  parms$response_column <- args$y

  if (!missing(model_id))
    parms$model_id <- model_id
  if (!missing(validation_frame))
    parms$validation_frame <- validation_frame
  if (!missing(seed))
    parms$seed <- seed
  if (!missing(keep_cross_validation_models))
    parms$keep_cross_validation_models <- keep_cross_validation_models
  if (!missing(keep_cross_validation_predictions))
    parms$keep_cross_validation_predictions <- keep_cross_validation_predictions
  if (!missing(keep_cross_validation_fold_assignment))
    parms$keep_cross_validation_fold_assignment <- keep_cross_validation_fold_assignment
  if (!missing(fold_assignment))
    parms$fold_assignment <- fold_assignment
  if (!missing(fold_column))
    parms$fold_column <- fold_column
  if (!missing(ignore_const_cols))
    parms$ignore_const_cols <- ignore_const_cols
  if (!missing(score_each_iteration))
    parms$score_each_iteration <- score_each_iteration
  if (!missing(offset_column))
    parms$offset_column <- offset_column
  if (!missing(weights_column))
    parms$weights_column <- weights_column
  if (!missing(standardize))
    parms$standardize <- standardize
  if (!missing(distribution))
    parms$distribution <- distribution
  if (!missing(plug_values))
    parms$plug_values <- plug_values
  if (!missing(max_iterations))
    parms$max_iterations <- max_iterations
  if (!missing(stopping_rounds))
    parms$stopping_rounds <- stopping_rounds
  if (!missing(stopping_metric))
    parms$stopping_metric <- stopping_metric
  if (!missing(stopping_tolerance))
    parms$stopping_tolerance <- stopping_tolerance
  if (!missing(balance_classes))
    parms$balance_classes <- balance_classes
  if (!missing(class_sampling_factors))
    parms$class_sampling_factors <- class_sampling_factors
  if (!missing(max_after_balance_size))
    parms$max_after_balance_size <- max_after_balance_size
  if (!missing(max_runtime_secs))
    parms$max_runtime_secs <- max_runtime_secs
  if (!missing(custom_metric_func))
    parms$custom_metric_func <- custom_metric_func
  if (!missing(auc_type))
    parms$auc_type <- auc_type
  if (!missing(infogram_algorithm))
    parms$infogram_algorithm <- infogram_algorithm
  if (!missing(protected_columns))
    parms$protected_columns <- protected_columns
  if (!missing(cmi_threshold))
    parms$cmi_threshold <- cmi_threshold
  if (!missing(relevance_threshold))
    parms$relevance_threshold <- relevance_threshold
  if (!missing(data_fraction))
    parms$data_fraction <- data_fraction
  if (!missing(top_n_features))
    parms$top_n_features <- top_n_features
  if (!missing(compute_p_values))
    parms$compute_p_values <- compute_p_values

  if (!missing(infogram_algorithm_params))
      parms$infogram_algorithm_params <- as.character(toJSON(infogram_algorithm_params, pretty = TRUE))
  h2o.show_progress() # enable progress bar explicitly

  # Error check and build model
  model <- .h2o.modelJob('infogram', parms, h2oRestApiVersion=3, verbose=FALSE)

  # Convert infogram_algorithm_params back to list if not NULL, added after obtaining model
  if (!missing(infogram_algorithm_params)) {
      model@parameters$infogram_algorithm_params <- list(fromJSON(model@parameters$infogram_algorithm_params))[[1]] #Need the `[[ ]]` to avoid a nested list
  }
  infogram_model <- new("H2OInfogramModel", model_id=model@model_id)       
  model <- infogram_model                
  return(model)
}
.h2o.train_segments_infogram <- function(x,
                                         y,
                                         training_frame,
                                         validation_frame = NULL,
                                         seed = -1,
                                         keep_cross_validation_models = TRUE,
                                         keep_cross_validation_predictions = FALSE,
                                         keep_cross_validation_fold_assignment = FALSE,
                                         fold_assignment = c("AUTO", "Random", "Modulo", "Stratified"),
                                         fold_column = NULL,
                                         ignore_const_cols = TRUE,
                                         score_each_iteration = FALSE,
                                         offset_column = NULL,
                                         weights_column = NULL,
                                         standardize = FALSE,
                                         distribution = c("AUTO", "bernoulli", "multinomial", "gaussian", "poisson", "gamma", "tweedie", "laplace", "quantile", "huber"),
                                         plug_values = NULL,
                                         max_iterations = 0,
                                         stopping_rounds = 0,
                                         stopping_metric = c("AUTO", "deviance", "logloss", "MSE", "RMSE", "MAE", "RMSLE", "AUC", "AUCPR", "lift_top_group", "misclassification", "mean_per_class_error", "custom", "custom_increasing"),
                                         stopping_tolerance = 0.001,
                                         balance_classes = FALSE,
                                         class_sampling_factors = NULL,
                                         max_after_balance_size = 5.0,
                                         max_runtime_secs = 0,
                                         custom_metric_func = NULL,
                                         auc_type = c("AUTO", "NONE", "MACRO_OVR", "WEIGHTED_OVR", "MACRO_OVO", "WEIGHTED_OVO"),
                                         infogram_algorithm = c("AUTO", "deeplearning", "drf", "gbm", "glm", "xgboost"),
                                         infogram_algorithm_params = NULL,
                                         protected_columns = NULL,
                                         cmi_threshold = 0.1,
                                         relevance_threshold = 0.1,
                                         data_fraction = 1,
                                         top_n_features = 50,
                                         compute_p_values = FALSE,
                                         segment_columns = NULL,
                                         segment_models_id = NULL,
                                         parallelism = 1)
{
  # formally define variables that were excluded from function parameters
  model_id <- NULL
  verbose <- NULL
  destination_key <- NULL
  # Validate required training_frame first and other frame args: should be a valid key or an H2OFrame object
  training_frame <- .validate.H2OFrame(training_frame, required=TRUE)
  validation_frame <- .validate.H2OFrame(validation_frame, required=FALSE)

  # Validate other required args
  # If x is missing, then assume user wants to use all columns as features.
  if (missing(x)) {
     if (is.numeric(y)) {
         x <- setdiff(col(training_frame), y)
     } else {
         x <- setdiff(colnames(training_frame), y)
     }
  }

  # Build parameter list to send to model builder
  parms <- list()
  parms$training_frame <- training_frame
  args <- .verify_dataxy(training_frame, x, y)
  if( !missing(offset_column) && !is.null(offset_column))  args$x_ignore <- args$x_ignore[!( offset_column == args$x_ignore )]
  if( !missing(weights_column) && !is.null(weights_column)) args$x_ignore <- args$x_ignore[!( weights_column == args$x_ignore )]
  if( !missing(fold_column) && !is.null(fold_column)) args$x_ignore <- args$x_ignore[!( fold_column == args$x_ignore )]
  parms$ignored_columns <- args$x_ignore
  parms$response_column <- args$y

  if (!missing(validation_frame))
    parms$validation_frame <- validation_frame
  if (!missing(seed))
    parms$seed <- seed
  if (!missing(keep_cross_validation_models))
    parms$keep_cross_validation_models <- keep_cross_validation_models
  if (!missing(keep_cross_validation_predictions))
    parms$keep_cross_validation_predictions <- keep_cross_validation_predictions
  if (!missing(keep_cross_validation_fold_assignment))
    parms$keep_cross_validation_fold_assignment <- keep_cross_validation_fold_assignment
  if (!missing(fold_assignment))
    parms$fold_assignment <- fold_assignment
  if (!missing(fold_column))
    parms$fold_column <- fold_column
  if (!missing(ignore_const_cols))
    parms$ignore_const_cols <- ignore_const_cols
  if (!missing(score_each_iteration))
    parms$score_each_iteration <- score_each_iteration
  if (!missing(offset_column))
    parms$offset_column <- offset_column
  if (!missing(weights_column))
    parms$weights_column <- weights_column
  if (!missing(standardize))
    parms$standardize <- standardize
  if (!missing(distribution))
    parms$distribution <- distribution
  if (!missing(plug_values))
    parms$plug_values <- plug_values
  if (!missing(max_iterations))
    parms$max_iterations <- max_iterations
  if (!missing(stopping_rounds))
    parms$stopping_rounds <- stopping_rounds
  if (!missing(stopping_metric))
    parms$stopping_metric <- stopping_metric
  if (!missing(stopping_tolerance))
    parms$stopping_tolerance <- stopping_tolerance
  if (!missing(balance_classes))
    parms$balance_classes <- balance_classes
  if (!missing(class_sampling_factors))
    parms$class_sampling_factors <- class_sampling_factors
  if (!missing(max_after_balance_size))
    parms$max_after_balance_size <- max_after_balance_size
  if (!missing(max_runtime_secs))
    parms$max_runtime_secs <- max_runtime_secs
  if (!missing(custom_metric_func))
    parms$custom_metric_func <- custom_metric_func
  if (!missing(auc_type))
    parms$auc_type <- auc_type
  if (!missing(infogram_algorithm))
    parms$infogram_algorithm <- infogram_algorithm
  if (!missing(protected_columns))
    parms$protected_columns <- protected_columns
  if (!missing(cmi_threshold))
    parms$cmi_threshold <- cmi_threshold
  if (!missing(relevance_threshold))
    parms$relevance_threshold <- relevance_threshold
  if (!missing(data_fraction))
    parms$data_fraction <- data_fraction
  if (!missing(top_n_features))
    parms$top_n_features <- top_n_features
  if (!missing(compute_p_values))
    parms$compute_p_values <- compute_p_values

  if (!missing(infogram_algorithm_params))
      parms$infogram_algorithm_params <- as.character(toJSON(infogram_algorithm_params, pretty = TRUE))
  h2o.show_progress() # enable progress bar explicitly

  # Build segment-models specific parameters
  segment_parms <- list()
  if (!missing(segment_columns))
    segment_parms$segment_columns <- segment_columns
  if (!missing(segment_models_id))
    segment_parms$segment_models_id <- segment_models_id
  segment_parms$parallelism <- parallelism

  # Error check and build segment models
  segment_models <- .h2o.segmentModelsJob('infogram', segment_parms, parms, h2oRestApiVersion=3)
  return(segment_models)
}


#' Extract the admissible attributes/predictors out of the H2O Infogram Model.
#'
#' @param model an H2OInfogramModel.
#' @export 
h2o.get_admissible_attributes<-function(model) {
  if ( is(model, "H2OInfogramModel") && (model@algorithm=='infogram'))
    return(model@admissible_features)
}

